# -*- coding: utf-8 -*-
"""loanprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a515MBBGXGh67I6fDRjjlvjNqUW53MCo
"""

import seaborn as sns
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import random

#creating the dataframe
df = pd.read_csv("train.csv")

#checking for null values
df.isnull().sum()

#checking the count of Male and Female for preprocessing 
print(sns.countplot(x=df["Gender"]))

#Because Men are dominant in terms on their frequency so we are filling null values with male
df["Gender"].fillna("Male", inplace =True)

#We are filling the null values randomly for dependants greater than 3 using numpy.random
for i in range(len(df["Dependents"])):
  if(df["Dependents"][i] == "3+"):
    df["Dependents"][i] = str(random.randrange(4, 8))

df["Dependents1"] = np.nan

# storing 0 to replace null values in dependents
df['Dependents'] = df['Dependents'].replace(np.nan, 0)

for i in range(len(df["Dependents"])):
    df["Dependents1"][i] = int(df["Dependents"][i])

# We are deleting the previous column as we have added a new colum
df.drop(["Dependents"], axis = 1, inplace =True)

#checking count for married and unmarried
sns.countplot(x=df["Married"])
#Because Married are  more so we are replacing null values with Married
df["Married"].fillna("Yes", inplace = True)
print(df["Married"].isnull().sum())

#checking count for Self_Employed
sns.countplot(x=df["Self_Employed"])
#because more people are not self-employed so replacing null values with  No
df["Self_Employed"].fillna("No", inplace = True)
print(df["Self_Employed"].isnull().sum())

#Replacing null values in Loan amount with the average 
mean1 = df["LoanAmount"].mean()
df["LoanAmount"].fillna(mean1, inplace = True)

#Replacing null values in Loan_Amount_Term with the average 
mean2 = df["Loan_Amount_Term"].mean()
df["Loan_Amount_Term"].fillna(mean2, inplace = True)

#Replacing null values in Credit_History with the average 
mean3 = df["Credit_History"].mean()
df["Credit_History"].fillna(mean3, inplace = True)

#using one hot encoding to convert categorical values in gender to integer values
Gender = pd.get_dummies(df['Gender'],drop_first=True)
# dropping the previous gender column
df.drop("Gender", axis =1, inplace = True)

#using one hot encoding to convert categorical values in Married to integer values
Married = pd.get_dummies(df['Married'],drop_first=True, prefix='married')
# dropping the previous  Married column
df.drop("Married", axis =1, inplace = True)

#using one hot encoding to convert categorical values in Loan_Status to integer values
Loan_Status = pd.get_dummies(df['Loan_Status'],drop_first=True)
# dropping the previous Loan_Status column
df.drop("Loan_Status",axis =1, inplace = True)

#using one hot encoding to convert categorical values in Education to integer values
Education = pd.get_dummies(df['Education'],drop_first=True,prefix='graduate')
# dropping the previous Education column
df.drop("Education",axis =1, inplace = True)

#using one hot encoding to convert categorical values in Self_Employed to integer values
Self_Employed = pd.get_dummies(df['Self_Employed'],drop_first=True)
# dropping the previous Education column
df.drop("Self_Employed",axis =1, inplace = True)

#using one hot encoding to convert categorical values in Property_Area to integer values
Property_Area = pd.get_dummies(df['Property_Area'])
# dropping the previous Property_Area column
df.drop("Property_Area", axis =1,inplace = True)

# Adding the newly created columns back to dataframe
df = pd.concat([df, Gender, Married, Loan_Status, Education, Self_Employed, Property_Area], axis =1)

#checking the correlation
import matplotlib.pyplot as plt
plt.figure(figsize=(20,20))
impodata_correlation = df.corr()
sns.heatmap(impodata_correlation,annot=True)

import joblib

#dropping loan id as it is not important for prediction
df.drop("Loan_ID",axis=1,inplace = True)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('Y',axis=1), df['Y'], test_size=0.20, random_state=101)
from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train, y_train)
predictions=logmodel.predict(X_test)
filename = "finalized_model.sav"
joblib.dump(logmodel, filename)

from sklearn.metrics import classification_report
print( classification_report(y_test, predictions))

# from sklearn.tree import DecisionTreeClassifier
# dtree=DecisionTreeClassifier()
# dtree.fit( X_train, y_train)
# predictions=dtree.predict(X_test)
# print( classification_report(y_test, predictions))

# from sklearn.svm import SVC
# model=SVC()
# model.fit(X_train, y_train)
# prediction=model.predict(X_test)
# print(confusion_matrix(prediction, y_test))
# print(classification_report(prediction, y_test))

# from sklearn.neighbors import KNeighborsClassifier
# knn=KNeighborsClassifier(n_neighbors=1)
# knn.fit(X_train, y_train)
# pred=knn.predict(X_test)
# print(classification_report(pred, y_test))